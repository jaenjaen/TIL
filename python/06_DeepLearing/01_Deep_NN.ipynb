{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch 에서의 Gradient Descent\n",
    "    - 머신러닝 과정에서 선형회귀(Linear Regression)에 대해 간단히 살펴 보았음.\n",
    "    - 이번에는 파이토치를 이용하여 선형회귀(Linear Regression)을 위한 연산 그래프를 만들고, 경사하강법을 이해하는 코드를\n",
    "    다시 작성해볼 것임.\n",
    "    - 파이터치에서는 데이터의 기본단위 텐서(Tensor)를 사용한다.\n",
    "    Numpy에서 np.array([1,2,3])과 동일한 것으로 이러한 텐서를 연산의 기본단위로 사용한다.\n",
    "    - 텐서 생성 방법: torch.tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision #pytorch lib에서 vision은 img processing에 특화된 라이브러리\n",
    "import torch.nn as nn #nn: neural Network \n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms #transforms는 Data aumentation을 위한것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.Tensor(2,3)\n",
    "X #0에 가까운 값이 랜덤으로 입력됨\n",
    "X.shape #== X.size\n",
    "\n",
    "X = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  tensor(25.)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-bd24d541cd1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#3. 가장 가파른 기울기로 하강하는 weight를 찾음 == 가장 낮은 loss값을 구하는 weight를 찾음 -> 미분 편미분 / backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m '''\n\u001b[0;32m     33\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m->\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\lje\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\lje\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "'''\n",
    "tensor() 인자값으로 data.dtype,device,requires_grad등이 있따.\n",
    "dtype == data값의 타입지정\n",
    "device == 텐서를 어느 기기에 올릴 것인지 지정할 수 있음 (CPU? GPU? - ANN의 경우 layer가 하나이기때문에 연산양이 적어 CPU로 돌릴 수 있음)\n",
    "requires_grad == 텐서에 기울기를 지정할지 여부 (False) \n",
    "    - 미분 사용 여부 (안하면 backpropargation 진행이 안되서 학습이 안됨)\n",
    "    - 하지만 TestData는 학습할 필요가 없으니까 False로 넣으면 됨\n",
    "    - True의 경우 편미분을 해야되기 때문에 메모리를 굉장히 많이 잡아먹고 있음 그래서 False면 비교적 메모리를 적게 가져감\n",
    "'''\n",
    "\n",
    "# x = torch.tensor(data=[2.0,3.0],requires_grad = True)\n",
    "x = torch.tensor(data=[2.0,3.0]) #밑에 loss.backword()에서 error\n",
    "x\n",
    "\n",
    "y = x**2\n",
    "y\n",
    "\n",
    "#1.예측값 출력\n",
    "pred = 2*y+3 ## 2(x^2)+3\n",
    "pred\n",
    "\n",
    "target = torch.tensor([3.0,4.0])\n",
    "\n",
    "#2. loss값을 구한다\n",
    "loss = torch.sum(torch.abs(pred-target)) # (11-3) +(21-4) ==25 loss굉장히 높음\n",
    "loss #25나옴 \n",
    "print('Loss: ',loss)\n",
    "\n",
    "#3. 가장 가파른 기울기로 하강하는 weight를 찾음 == 가장 낮은 loss값을 구하는 weight를 찾음 -> 미분 편미분 / backpropagation\n",
    "\n",
    "loss.backward()\n",
    "'''\n",
    "x.grad --> dy/dw = x\n",
    "w.grad --> dy/dx = w\n",
    "'''\n",
    "print(x.grad,pred.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Type - tensor(), as_tensor()\n",
    "    - Numpy 배열을 torch.Tensor 변형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "int32\n",
      "<class 'numpy.ndarray'>\n",
      "****************************************\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "<class 'torch.Tensor'>\n",
      "****************************************\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "<class 'torch.Tensor'>\n",
      "****************************************\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "pytorch의 텐서 자료형은 numpy의 array()와 동일한 것으로 이해\n",
    "'''\n",
    "\n",
    "#1.\n",
    "li = np.array([[1,2],[3,4]])\n",
    "print(li)\n",
    "print(li.dtype)\n",
    "print(type(li))\n",
    "\n",
    "\n",
    "print('*'*40)\n",
    "\n",
    "li_torch = torch.tensor(li)\n",
    "print(li_torch)\n",
    "print(type(li_torch))\n",
    "\n",
    "print('*'*40)\n",
    "\n",
    "li_as_torch = torch.as_tensor(li)\n",
    "print(li_as_torch)\n",
    "print(type(li_as_torch))\n",
    "\n",
    "print('*'*40)\n",
    "#반대로 텐서를 numpy배열로 변형\n",
    "li_tensor_numpy = li_as_torch.numpy()\n",
    "print(li_tensor_numpy)\n",
    "print(type(li_tensor_numpy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
